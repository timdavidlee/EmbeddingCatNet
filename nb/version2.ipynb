{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils import load_wids_xy_data, get_mappers\n",
    "from EmbeddingModelV2 import EmbeddingModel, train_model\n",
    "\n",
    "DATA_DIR = '/Users/timlee/data/wids/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data ...\n",
      "loading test data ...\n",
      "complete ...\n",
      "formatting ...\n",
      "imputing missing values ...\n",
      "(18255, 1122) (18255,) (27285, 1122)\n"
     ]
    }
   ],
   "source": [
    "X, y, X_test = load_wids_xy_data(DATA_DIR, target='is_female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting to category ...\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "calculating cardinality\n",
      "remapping columns to int\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "cat_cols = X.columns\n",
    "emb_cols = ['DG3', 'DG4']\n",
    "X_mapped, mappers, emb_szs, idx2col, col2idx = get_mappers(X, cat_cols, emb_cols)\n",
    "\n",
    "cat_onehot_cols = X_mapped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emb feats: 2\n",
      "total embedding parameters 12\n"
     ]
    }
   ],
   "source": [
    "em = EmbeddingModel(emb_szs=emb_szs,\n",
    "                    cat_cols=cat_onehot_cols,\n",
    "                    idx2col=idx2col, \n",
    "                    col2idx=col2idx,\n",
    "                    layer_sizes=[500,100], \n",
    "                    output_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bz = 50\n",
    "X_tensor = torch.from_numpy(X_mapped.head(18200).as_matrix())\n",
    "y_tensor = torch.from_numpy(y[:18200]).view(-1,1)\n",
    "\n",
    "train = data_utils.TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=bz, shuffle=True)\n",
    "loss_fn = torch.nn.BCELoss(size_average=False)\n",
    "params = {\n",
    "    'weight_decay': 0.01,\n",
    "    'n_epoches': 2,\n",
    "    'learning_rate': 0.01,\n",
    "    'ml_type': 'binary'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.010000\n",
      "[1/2] - 1200/18200 loss: 25.528437, acc: 0.757600\n",
      "[1/2] - 2450/18200 loss: 19.337873, acc: 0.833600\n",
      "[1/2] - 3700/18200 loss: 18.758203, acc: 0.835200\n",
      "[1/2] - 4950/18200 loss: 17.701558, acc: 0.847200\n",
      "[1/2] - 6200/18200 loss: 15.136698, acc: 0.873600\n",
      "[1/2] - 7450/18200 loss: 15.061308, acc: 0.874400\n",
      "[1/2] - 8700/18200 loss: 15.817006, acc: 0.874400\n",
      "[1/2] - 9950/18200 loss: 15.926250, acc: 0.864000\n",
      "[1/2] - 11200/18200 loss: 14.881642, acc: 0.875200\n",
      "[1/2] - 12450/18200 loss: 15.016040, acc: 0.878400\n",
      "[1/2] - 13700/18200 loss: 14.973339, acc: 0.874400\n",
      "[1/2] - 14950/18200 loss: 14.955606, acc: 0.876800\n",
      "[1/2] - 16200/18200 loss: 15.689525, acc: 0.868800\n",
      "[1/2] - 17450/18200 loss: 14.206887, acc: 0.873600\n",
      "[2/2] - 1200/18200 loss: 14.373886, acc: 0.884000\n",
      "[2/2] - 2450/18200 loss: 15.510150, acc: 0.868800\n",
      "[2/2] - 3700/18200 loss: 13.434170, acc: 0.898400\n",
      "[2/2] - 4950/18200 loss: 14.609803, acc: 0.869600\n",
      "[2/2] - 6200/18200 loss: 15.039789, acc: 0.877600\n",
      "[2/2] - 7450/18200 loss: 14.795921, acc: 0.880800\n",
      "[2/2] - 8700/18200 loss: 15.111237, acc: 0.872800\n",
      "[2/2] - 9950/18200 loss: 15.725057, acc: 0.868000\n",
      "[2/2] - 11200/18200 loss: 14.239619, acc: 0.884800\n",
      "[2/2] - 12450/18200 loss: 14.421112, acc: 0.880800\n",
      "[2/2] - 13700/18200 loss: 15.173915, acc: 0.881600\n",
      "[2/2] - 14950/18200 loss: 13.742547, acc: 0.887200\n",
      "[2/2] - 16200/18200 loss: 14.152919, acc: 0.876800\n",
      "[2/2] - 17450/18200 loss: 13.559651, acc: 0.889600\n",
      "learning rate 0.003000\n",
      "[1/2] - 1200/18200 loss: 13.617659, acc: 0.880800\n",
      "[1/2] - 2450/18200 loss: 13.435336, acc: 0.893600\n",
      "[1/2] - 3700/18200 loss: 12.113865, acc: 0.902400\n",
      "[1/2] - 4950/18200 loss: 13.549406, acc: 0.885600\n",
      "[1/2] - 6200/18200 loss: 12.928197, acc: 0.888000\n",
      "[1/2] - 7450/18200 loss: 12.455815, acc: 0.896000\n",
      "[1/2] - 8700/18200 loss: 11.951916, acc: 0.901600\n",
      "[1/2] - 9950/18200 loss: 12.198947, acc: 0.906400\n",
      "[1/2] - 11200/18200 loss: 11.990849, acc: 0.897600\n",
      "[1/2] - 12450/18200 loss: 13.379563, acc: 0.892000\n",
      "[1/2] - 13700/18200 loss: 12.912883, acc: 0.898400\n",
      "[1/2] - 14950/18200 loss: 14.018778, acc: 0.868000\n",
      "[1/2] - 16200/18200 loss: 13.658495, acc: 0.893600\n",
      "[1/2] - 17450/18200 loss: 11.671351, acc: 0.904000\n",
      "[2/2] - 1200/18200 loss: 11.899355, acc: 0.897600\n",
      "[2/2] - 2450/18200 loss: 11.519716, acc: 0.914400\n",
      "[2/2] - 3700/18200 loss: 11.533763, acc: 0.904800\n",
      "[2/2] - 4950/18200 loss: 12.851229, acc: 0.890400\n",
      "[2/2] - 6200/18200 loss: 12.653977, acc: 0.890400\n",
      "[2/2] - 7450/18200 loss: 12.370088, acc: 0.895200\n",
      "[2/2] - 8700/18200 loss: 12.460896, acc: 0.901600\n",
      "[2/2] - 9950/18200 loss: 12.457043, acc: 0.901600\n",
      "[2/2] - 11200/18200 loss: 12.153794, acc: 0.893600\n",
      "[2/2] - 12450/18200 loss: 13.407028, acc: 0.890400\n",
      "[2/2] - 13700/18200 loss: 12.419342, acc: 0.888000\n",
      "[2/2] - 14950/18200 loss: 12.485131, acc: 0.902400\n",
      "[2/2] - 16200/18200 loss: 13.008255, acc: 0.888800\n",
      "[2/2] - 17450/18200 loss: 14.182027, acc: 0.874400\n",
      "learning rate 0.001000\n",
      "[1/2] - 1200/18200 loss: 10.927562, acc: 0.909600\n",
      "[1/2] - 2450/18200 loss: 10.721746, acc: 0.912000\n",
      "[1/2] - 3700/18200 loss: 10.779296, acc: 0.908000\n",
      "[1/2] - 4950/18200 loss: 11.319270, acc: 0.904000\n",
      "[1/2] - 6200/18200 loss: 10.761786, acc: 0.920800\n",
      "[1/2] - 7450/18200 loss: 11.546232, acc: 0.912000\n",
      "[1/2] - 8700/18200 loss: 11.121436, acc: 0.912000\n",
      "[1/2] - 9950/18200 loss: 11.478232, acc: 0.909600\n",
      "[1/2] - 11200/18200 loss: 12.795829, acc: 0.895200\n",
      "[1/2] - 12450/18200 loss: 10.850021, acc: 0.901600\n",
      "[1/2] - 13700/18200 loss: 11.556483, acc: 0.920000\n",
      "[1/2] - 14950/18200 loss: 11.121337, acc: 0.897600\n",
      "[1/2] - 16200/18200 loss: 13.467367, acc: 0.884800\n",
      "[1/2] - 17450/18200 loss: 12.558497, acc: 0.885600\n",
      "[2/2] - 1200/18200 loss: 11.964185, acc: 0.903200\n",
      "[2/2] - 2450/18200 loss: 10.599750, acc: 0.916000\n",
      "[2/2] - 3700/18200 loss: 10.526021, acc: 0.914400\n",
      "[2/2] - 4950/18200 loss: 11.096210, acc: 0.909600\n",
      "[2/2] - 6200/18200 loss: 12.360814, acc: 0.897600\n",
      "[2/2] - 7450/18200 loss: 10.183254, acc: 0.916800\n",
      "[2/2] - 8700/18200 loss: 11.085695, acc: 0.899200\n",
      "[2/2] - 9950/18200 loss: 10.784374, acc: 0.910400\n",
      "[2/2] - 11200/18200 loss: 10.712534, acc: 0.905600\n",
      "[2/2] - 12450/18200 loss: 12.473735, acc: 0.903200\n",
      "[2/2] - 13700/18200 loss: 10.781072, acc: 0.912800\n",
      "[2/2] - 14950/18200 loss: 10.957407, acc: 0.904000\n",
      "[2/2] - 16200/18200 loss: 12.938633, acc: 0.893600\n",
      "[2/2] - 17450/18200 loss: 10.941912, acc: 0.900800\n",
      "learning rate 0.000300\n",
      "[1/2] - 1200/18200 loss: 9.822755, acc: 0.916800\n",
      "[1/2] - 2450/18200 loss: 10.227165, acc: 0.922400\n",
      "[1/2] - 3700/18200 loss: 10.170607, acc: 0.924800\n",
      "[1/2] - 4950/18200 loss: 12.099304, acc: 0.899200\n",
      "[1/2] - 6200/18200 loss: 9.932810, acc: 0.916800\n",
      "[1/2] - 7450/18200 loss: 11.249520, acc: 0.908800\n",
      "[1/2] - 8700/18200 loss: 9.667648, acc: 0.932800\n",
      "[1/2] - 9950/18200 loss: 10.802987, acc: 0.909600\n",
      "[1/2] - 11200/18200 loss: 9.578200, acc: 0.924800\n",
      "[1/2] - 12450/18200 loss: 10.242657, acc: 0.912000\n",
      "[1/2] - 13700/18200 loss: 11.089505, acc: 0.910400\n",
      "[1/2] - 14950/18200 loss: 9.873851, acc: 0.917600\n",
      "[1/2] - 16200/18200 loss: 11.515243, acc: 0.907200\n",
      "[1/2] - 17450/18200 loss: 9.781178, acc: 0.924800\n",
      "[2/2] - 1200/18200 loss: 8.820387, acc: 0.928800\n",
      "[2/2] - 2450/18200 loss: 9.593097, acc: 0.927200\n",
      "[2/2] - 3700/18200 loss: 11.238823, acc: 0.917600\n",
      "[2/2] - 4950/18200 loss: 9.987924, acc: 0.921600\n",
      "[2/2] - 6200/18200 loss: 10.068283, acc: 0.920000\n",
      "[2/2] - 7450/18200 loss: 10.272887, acc: 0.920000\n",
      "[2/2] - 8700/18200 loss: 9.796394, acc: 0.921600\n",
      "[2/2] - 9950/18200 loss: 10.403795, acc: 0.920000\n",
      "[2/2] - 11200/18200 loss: 9.800074, acc: 0.921600\n",
      "[2/2] - 12450/18200 loss: 11.240894, acc: 0.899200\n",
      "[2/2] - 13700/18200 loss: 9.480318, acc: 0.925600\n",
      "[2/2] - 14950/18200 loss: 9.969939, acc: 0.915200\n",
      "[2/2] - 16200/18200 loss: 11.302942, acc: 0.906400\n",
      "[2/2] - 17450/18200 loss: 9.970724, acc: 0.916800\n",
      "learning rate 0.000100\n",
      "[1/2] - 1200/18200 loss: 9.309876, acc: 0.924800\n",
      "[1/2] - 2450/18200 loss: 10.504979, acc: 0.908800\n",
      "[1/2] - 3700/18200 loss: 9.027434, acc: 0.925600\n",
      "[1/2] - 4950/18200 loss: 9.125999, acc: 0.936000\n",
      "[1/2] - 6200/18200 loss: 11.073367, acc: 0.902400\n",
      "[1/2] - 7450/18200 loss: 9.994001, acc: 0.923200\n",
      "[1/2] - 8700/18200 loss: 9.519318, acc: 0.923200\n",
      "[1/2] - 9950/18200 loss: 9.678712, acc: 0.912800\n",
      "[1/2] - 11200/18200 loss: 10.328620, acc: 0.917600\n",
      "[1/2] - 12450/18200 loss: 9.284901, acc: 0.920800\n",
      "[1/2] - 13700/18200 loss: 9.509443, acc: 0.922400\n",
      "[1/2] - 14950/18200 loss: 10.739400, acc: 0.912000\n",
      "[1/2] - 16200/18200 loss: 9.380283, acc: 0.928000\n",
      "[1/2] - 17450/18200 loss: 9.953827, acc: 0.921600\n",
      "[2/2] - 1200/18200 loss: 9.472959, acc: 0.925600\n",
      "[2/2] - 2450/18200 loss: 10.146638, acc: 0.920800\n",
      "[2/2] - 3700/18200 loss: 8.897438, acc: 0.928000\n",
      "[2/2] - 4950/18200 loss: 8.810203, acc: 0.934400\n",
      "[2/2] - 6200/18200 loss: 9.549781, acc: 0.924000\n",
      "[2/2] - 7450/18200 loss: 9.647968, acc: 0.920000\n",
      "[2/2] - 8700/18200 loss: 9.643209, acc: 0.915200\n",
      "[2/2] - 9950/18200 loss: 10.467612, acc: 0.914400\n",
      "[2/2] - 11200/18200 loss: 10.649597, acc: 0.921600\n",
      "[2/2] - 12450/18200 loss: 9.210256, acc: 0.921600\n",
      "[2/2] - 13700/18200 loss: 11.316077, acc: 0.917600\n",
      "[2/2] - 14950/18200 loss: 9.221989, acc: 0.925600\n",
      "[2/2] - 16200/18200 loss: 9.288396, acc: 0.917600\n",
      "[2/2] - 17450/18200 loss: 8.704146, acc: 0.936000\n"
     ]
    }
   ],
   "source": [
    "train_model(em, train_loader, loss_fn, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to Predict Level of Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import kaiming_normal\n",
    "\n",
    "y_mapper = {\n",
    "    1:0,\n",
    "    2:1,\n",
    "    3:2,\n",
    "    4:3,\n",
    "    5:4,\n",
    "    6:5,\n",
    "    7:6,\n",
    "    8:7,\n",
    "    9:8,\n",
    "    10:9,\n",
    "    11:10,\n",
    "    12:11,\n",
    "    96:12,\n",
    "    99:13\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data ...\n",
      "loading test data ...\n",
      "complete ...\n",
      "formatting ...\n",
      "imputing missing values ...\n",
      "(18255, 1121) (18255,) (27285, 1121) (27285,)\n"
     ]
    }
   ],
   "source": [
    "X, y, X_test, y_test = load_wids_xy_data(DATA_DIR, target='DG4')\n",
    "y = np.array([y_mapper[v] for v in y])\n",
    "y_test = np.array([y_mapper[v] for v in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45540, 1121), (45540,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_join = pd.concat([X, X_test])\n",
    "y_join = np.concatenate([y, y_test])\n",
    "X_join.shape, y_join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting to category ...\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "calculating cardinality\n",
      "remapping columns to int\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "cat_cols = X.columns\n",
    "emb_cols = cat_cols \n",
    "X_mapped, mappers, emb_szs, idx2col, col2idx = get_mappers(X, cat_cols, emb_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bz = 50\n",
    "n_classes = max(set(y_join))+1\n",
    "X_tensor = torch.from_numpy(X_mapped.head(45000).as_matrix())\n",
    "y_tensor = torch.from_numpy(y[:45000]).view(-1,1)\n",
    "y_1hot_tensor = torch.zeros([y_tensor.shape[0], int(n_classes)])\n",
    "y_1hot_tensor.scatter_(1, y_tensor, 1) \n",
    "\n",
    "train = data_utils.TensorDataset(X_tensor, y_1hot_tensor)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=bz, shuffle=True)\n",
    "loss_fn = torch.nn.MultiLabelSoftMarginLoss()\n",
    "params = {\n",
    "    'weight_decay': 0.01,\n",
    "    'n_epoches': 2,\n",
    "    'learning_rate': 0.01,\n",
    "    'ml_type':'multi',\n",
    "    'n_classes': n_classes\n",
    "}\n",
    "cat_onehot_cols = X_mapped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emb feats: 1121\n",
      "total embedding parameters 3494\n"
     ]
    }
   ],
   "source": [
    "em = EmbeddingModel(emb_szs=emb_szs,\n",
    "                    cat_cols=cat_cols,\n",
    "                    idx2col=idx2col, \n",
    "                    col2idx=col2idx,\n",
    "                    layer_sizes=[1000,300, 100], \n",
    "                    output_dim=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.010000\n",
      "[1/2] - 1200/18200 loss: 0.296072, acc: 0.156000\n",
      "[1/2] - 2450/18200 loss: 0.287184, acc: 0.289600\n",
      "[1/2] - 3700/18200 loss: 0.283537, acc: 0.336000\n",
      "[1/2] - 4950/18200 loss: 0.283725, acc: 0.352800\n",
      "[1/2] - 6200/18200 loss: 0.285810, acc: 0.354400\n",
      "[1/2] - 7450/18200 loss: 0.287987, acc: 0.348800\n",
      "[1/2] - 8700/18200 loss: 0.287494, acc: 0.388800\n",
      "[1/2] - 9950/18200 loss: 0.288795, acc: 0.316800\n",
      "[1/2] - 11200/18200 loss: 0.288188, acc: 0.311200\n",
      "[1/2] - 12450/18200 loss: 0.288168, acc: 0.356000\n",
      "[1/2] - 13700/18200 loss: 0.287874, acc: 0.363200\n",
      "[1/2] - 14950/18200 loss: 0.288887, acc: 0.317600\n",
      "[1/2] - 16200/18200 loss: 0.288068, acc: 0.348800\n",
      "[1/2] - 17450/18200 loss: 0.288274, acc: 0.340800\n",
      "[2/2] - 1200/18200 loss: 0.289014, acc: 0.330400\n",
      "[2/2] - 2450/18200 loss: 0.288992, acc: 0.353600\n",
      "[2/2] - 3700/18200 loss: 0.288761, acc: 0.343200\n",
      "[2/2] - 4950/18200 loss: 0.290257, acc: 0.334400\n",
      "[2/2] - 6200/18200 loss: 0.289255, acc: 0.342400\n",
      "[2/2] - 7450/18200 loss: 0.289408, acc: 0.316000\n",
      "[2/2] - 8700/18200 loss: 0.288765, acc: 0.353600\n",
      "[2/2] - 9950/18200 loss: 0.287971, acc: 0.352800\n",
      "[2/2] - 11200/18200 loss: 0.288968, acc: 0.368800\n",
      "[2/2] - 12450/18200 loss: 0.289813, acc: 0.357600\n",
      "[2/2] - 13700/18200 loss: 0.290478, acc: 0.311200\n",
      "[2/2] - 14950/18200 loss: 0.288598, acc: 0.356000\n",
      "[2/2] - 16200/18200 loss: 0.290353, acc: 0.305600\n",
      "[2/2] - 17450/18200 loss: 0.290175, acc: 0.309600\n",
      "learning rate 0.003000\n",
      "[1/2] - 1200/18200 loss: 0.288276, acc: 0.356000\n",
      "[1/2] - 2450/18200 loss: 0.287508, acc: 0.373600\n",
      "[1/2] - 3700/18200 loss: 0.287296, acc: 0.374400\n",
      "[1/2] - 4950/18200 loss: 0.286629, acc: 0.378400\n",
      "[1/2] - 6200/18200 loss: 0.287749, acc: 0.366400\n",
      "[1/2] - 7450/18200 loss: 0.287425, acc: 0.344800\n",
      "[1/2] - 8700/18200 loss: 0.287539, acc: 0.358400\n",
      "[1/2] - 9950/18200 loss: 0.287271, acc: 0.369600\n",
      "[1/2] - 11200/18200 loss: 0.286949, acc: 0.380800\n",
      "[1/2] - 12450/18200 loss: 0.287125, acc: 0.350400\n",
      "[1/2] - 13700/18200 loss: 0.286592, acc: 0.368800\n",
      "[1/2] - 14950/18200 loss: 0.286275, acc: 0.374400\n",
      "[1/2] - 16200/18200 loss: 0.286397, acc: 0.348800\n",
      "[1/2] - 17450/18200 loss: 0.287057, acc: 0.309600\n",
      "[2/2] - 1200/18200 loss: 0.288508, acc: 0.324800\n",
      "[2/2] - 2450/18200 loss: 0.287311, acc: 0.349600\n",
      "[2/2] - 3700/18200 loss: 0.287205, acc: 0.363200\n",
      "[2/2] - 4950/18200 loss: 0.286939, acc: 0.376800\n",
      "[2/2] - 6200/18200 loss: 0.287266, acc: 0.365600\n",
      "[2/2] - 7450/18200 loss: 0.285832, acc: 0.390400\n",
      "[2/2] - 8700/18200 loss: 0.287414, acc: 0.325600\n",
      "[2/2] - 9950/18200 loss: 0.288128, acc: 0.340800\n",
      "[2/2] - 11200/18200 loss: 0.288310, acc: 0.351200\n",
      "[2/2] - 12450/18200 loss: 0.287879, acc: 0.360800\n",
      "[2/2] - 13700/18200 loss: 0.287622, acc: 0.376000\n",
      "[2/2] - 14950/18200 loss: 0.287164, acc: 0.351200\n",
      "[2/2] - 16200/18200 loss: 0.286625, acc: 0.361600\n",
      "[2/2] - 17450/18200 loss: 0.287102, acc: 0.352800\n",
      "learning rate 0.001000\n",
      "[1/2] - 1200/18200 loss: 0.286922, acc: 0.353600\n",
      "[1/2] - 2450/18200 loss: 0.286317, acc: 0.362400\n",
      "[1/2] - 3700/18200 loss: 0.285261, acc: 0.384000\n",
      "[1/2] - 4950/18200 loss: 0.285486, acc: 0.379200\n",
      "[1/2] - 6200/18200 loss: 0.285641, acc: 0.374400\n",
      "[1/2] - 7450/18200 loss: 0.285833, acc: 0.380000\n",
      "[1/2] - 8700/18200 loss: 0.286113, acc: 0.366400\n",
      "[1/2] - 9950/18200 loss: 0.285051, acc: 0.389600\n",
      "[1/2] - 11200/18200 loss: 0.285253, acc: 0.367200\n",
      "[1/2] - 12450/18200 loss: 0.284999, acc: 0.393600\n",
      "[1/2] - 13700/18200 loss: 0.286379, acc: 0.365600\n",
      "[1/2] - 14950/18200 loss: 0.285515, acc: 0.389600\n",
      "[1/2] - 16200/18200 loss: 0.286378, acc: 0.383200\n",
      "[1/2] - 17450/18200 loss: 0.286033, acc: 0.376000\n",
      "[2/2] - 1200/18200 loss: 0.285958, acc: 0.380800\n",
      "[2/2] - 2450/18200 loss: 0.286110, acc: 0.369600\n",
      "[2/2] - 3700/18200 loss: 0.285310, acc: 0.383200\n",
      "[2/2] - 4950/18200 loss: 0.285316, acc: 0.385600\n",
      "[2/2] - 6200/18200 loss: 0.285578, acc: 0.372000\n",
      "[2/2] - 7450/18200 loss: 0.285247, acc: 0.398400\n",
      "[2/2] - 8700/18200 loss: 0.285757, acc: 0.372800\n",
      "[2/2] - 9950/18200 loss: 0.284793, acc: 0.392000\n",
      "[2/2] - 11200/18200 loss: 0.284991, acc: 0.358400\n",
      "[2/2] - 12450/18200 loss: 0.285050, acc: 0.379200\n",
      "[2/2] - 13700/18200 loss: 0.285245, acc: 0.361600\n",
      "[2/2] - 14950/18200 loss: 0.285171, acc: 0.360800\n",
      "[2/2] - 16200/18200 loss: 0.285263, acc: 0.390400\n",
      "[2/2] - 17450/18200 loss: 0.285640, acc: 0.381600\n",
      "learning rate 0.000300\n",
      "[1/2] - 1200/18200 loss: 0.284741, acc: 0.419200\n",
      "[1/2] - 2450/18200 loss: 0.285747, acc: 0.358400\n",
      "[1/2] - 3700/18200 loss: 0.284403, acc: 0.391200\n",
      "[1/2] - 4950/18200 loss: 0.284872, acc: 0.392000\n",
      "[1/2] - 6200/18200 loss: 0.284746, acc: 0.387200\n",
      "[1/2] - 7450/18200 loss: 0.284787, acc: 0.380000\n",
      "[1/2] - 8700/18200 loss: 0.284539, acc: 0.376000\n",
      "[1/2] - 9950/18200 loss: 0.284600, acc: 0.372800\n",
      "[1/2] - 11200/18200 loss: 0.285152, acc: 0.378400\n",
      "[1/2] - 12450/18200 loss: 0.283536, acc: 0.400000\n",
      "[1/2] - 13700/18200 loss: 0.283604, acc: 0.390400\n",
      "[1/2] - 14950/18200 loss: 0.284681, acc: 0.372800\n",
      "[1/2] - 16200/18200 loss: 0.284081, acc: 0.383200\n",
      "[1/2] - 17450/18200 loss: 0.283614, acc: 0.390400\n",
      "[2/2] - 1200/18200 loss: 0.283471, acc: 0.380800\n",
      "[2/2] - 2450/18200 loss: 0.284068, acc: 0.368000\n",
      "[2/2] - 3700/18200 loss: 0.284337, acc: 0.396800\n",
      "[2/2] - 4950/18200 loss: 0.283820, acc: 0.376000\n",
      "[2/2] - 6200/18200 loss: 0.283900, acc: 0.387200\n",
      "[2/2] - 7450/18200 loss: 0.284253, acc: 0.380000\n",
      "[2/2] - 8700/18200 loss: 0.283845, acc: 0.381600\n",
      "[2/2] - 9950/18200 loss: 0.284135, acc: 0.378400\n",
      "[2/2] - 11200/18200 loss: 0.284120, acc: 0.375200\n",
      "[2/2] - 12450/18200 loss: 0.283292, acc: 0.395200\n",
      "[2/2] - 13700/18200 loss: 0.284665, acc: 0.364000\n",
      "[2/2] - 14950/18200 loss: 0.284524, acc: 0.374400\n",
      "[2/2] - 16200/18200 loss: 0.284553, acc: 0.392000\n",
      "[2/2] - 17450/18200 loss: 0.283388, acc: 0.399200\n",
      "learning rate 0.000100\n",
      "[1/2] - 1200/18200 loss: 0.284272, acc: 0.389600\n",
      "[1/2] - 2450/18200 loss: 0.283583, acc: 0.394400\n",
      "[1/2] - 3700/18200 loss: 0.283648, acc: 0.383200\n",
      "[1/2] - 4950/18200 loss: 0.283784, acc: 0.364800\n",
      "[1/2] - 6200/18200 loss: 0.283414, acc: 0.418400\n",
      "[1/2] - 7450/18200 loss: 0.283812, acc: 0.374400\n",
      "[1/2] - 8700/18200 loss: 0.282709, acc: 0.401600\n",
      "[1/2] - 9950/18200 loss: 0.283490, acc: 0.368000\n",
      "[1/2] - 11200/18200 loss: 0.283366, acc: 0.375200\n",
      "[1/2] - 12450/18200 loss: 0.283856, acc: 0.395200\n",
      "[1/2] - 13700/18200 loss: 0.283714, acc: 0.390400\n",
      "[1/2] - 14950/18200 loss: 0.283272, acc: 0.394400\n",
      "[1/2] - 16200/18200 loss: 0.283740, acc: 0.368000\n",
      "[1/2] - 17450/18200 loss: 0.283136, acc: 0.388000\n",
      "[2/2] - 1200/18200 loss: 0.282554, acc: 0.416000\n",
      "[2/2] - 2450/18200 loss: 0.284151, acc: 0.367200\n",
      "[2/2] - 3700/18200 loss: 0.282903, acc: 0.392000\n",
      "[2/2] - 4950/18200 loss: 0.282930, acc: 0.407200\n",
      "[2/2] - 6200/18200 loss: 0.282556, acc: 0.395200\n",
      "[2/2] - 7450/18200 loss: 0.283419, acc: 0.393600\n",
      "[2/2] - 8700/18200 loss: 0.283174, acc: 0.389600\n",
      "[2/2] - 9950/18200 loss: 0.283092, acc: 0.399200\n",
      "[2/2] - 11200/18200 loss: 0.283451, acc: 0.384800\n",
      "[2/2] - 12450/18200 loss: 0.282978, acc: 0.408000\n",
      "[2/2] - 13700/18200 loss: 0.283615, acc: 0.377600\n",
      "[2/2] - 14950/18200 loss: 0.283147, acc: 0.381600\n",
      "[2/2] - 16200/18200 loss: 0.283514, acc: 0.374400\n",
      "[2/2] - 17450/18200 loss: 0.282854, acc: 0.394400\n"
     ]
    }
   ],
   "source": [
    "train_model(em, train_loader, loss_fn, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
