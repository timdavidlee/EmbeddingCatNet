{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules.loss import BCELoss\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import  kaiming_normal\n",
    "import torch.utils.data as data_utils\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xy_data():\n",
    "    print('loading data ...')\n",
    "    df = pd.read_csv('./train.csv', low_memory=False)\n",
    "    print('complete ...')\n",
    "\n",
    "    print('formatting ...')\n",
    "    drop_cols = [col for col in df.columns if 'OTHERS' in col] + [col for col in df.columns if 'REC' in col] + ['LN2_RIndLngBEOth', 'LN2_WIndLngBEOth', 'train_id']\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "    columns = [col for col in df.columns if col != 'is_female']\n",
    "    y = df['is_female'].values\n",
    "    X = df[columns].copy()\n",
    "\n",
    "\n",
    "    print('imputing missing values ...')\n",
    "    X.fillna(-1, inplace=True)\n",
    "    print(X.shape, y.shape)\n",
    "    return X, y\n",
    "\n",
    "def get_mappers(inputX):\n",
    "    \"\"\"\n",
    "    X_mapped, mappers, categorical_stats, emb_szs = get_mappers(X)\n",
    "    \"\"\"\n",
    "    X = inputX.copy()\n",
    "    mappers = {}\n",
    "    columns = X.columns\n",
    "    \n",
    "    print('converting to category ...')\n",
    "    for idx, col in enumerate(columns):\n",
    "        if idx % 100 == 0:\n",
    "            print(idx)\n",
    "        X[col] = X[col].astype('category')\n",
    "        mappers[col] = {labels: idx for idx, labels in enumerate(X[col].cat.categories)}\n",
    "\n",
    "    print('calculating cardinality')\n",
    "    categorical_stats = OrderedDict()\n",
    "    for col in X.columns:\n",
    "        categorical_stats[col] = len(X[col].cat.categories)+1\n",
    "\n",
    "    embedding_sizes = OrderedDict()\n",
    "    for ky, vl in categorical_stats.items():\n",
    "        embedding_sizes[ky] = (vl, min(50, (vl+1)//2))\n",
    "        \n",
    "    print('remapping columns to int')        \n",
    "    for col in columns:\n",
    "        X[col] = X[col].map(mappers[col])\n",
    "        \n",
    "    emb_szs = list(embedding_sizes.values())\n",
    "    input_feats_ct = len(embedding_sizes.keys())\n",
    "    print('complete')\n",
    "    \n",
    "    return X, mappers, categorical_stats, emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def norm_init_emb(embedding):\n",
    "    \"\"\" \n",
    "    Limit the initialization to 1/dim \n",
    "    Doesn't return anything\n",
    "    \"\"\"\n",
    "    vec_dim = embedding.weight.data.size(1)\n",
    "    sc = 2/(vec_dim + 1)\n",
    "    embedding.weight.data.uniform_(-sc,sc)\n",
    "    \n",
    "    \n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, emb_szs, layer_sizes, output_dim, drop_pct, emb_drop_pct):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        \n",
    "        # Number of layers\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        print('number of feats:% d' % len(emb_szs))\n",
    "        \n",
    "        # initialize the embeddings\n",
    "        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])        \n",
    "        self.total_emb_var_ct = 0        \n",
    "        for emb in self.embs:\n",
    "            norm_init_emb(emb)\n",
    "            self.total_emb_var_ct += emb.embedding_dim\n",
    "        \n",
    "        self.emb_drop = nn.Dropout(emb_drop_pct)\n",
    "        print('total embedding parameters %d' % self.total_emb_var_ct)\n",
    "        \n",
    "        \n",
    "        # initialize the layers, will make as many \n",
    "        # sub blocks as passed in by layer_size list \n",
    "        # ex. [n1, n2, n3] => 3 lin layer network + 1 output layer\n",
    "        self.seq_model = torch.nn.Sequential()\n",
    "        \n",
    "        layer_sizes = [self.total_emb_var_ct] + layer_sizes\n",
    "        for i in range(self.n_layers):\n",
    "            linlayer = nn.Linear(layer_sizes[i],layer_sizes[i+1])\n",
    "            kaiming_normal(linlayer.weight.data)            \n",
    "            self.seq_model.add_module(\"lin_%d\" % i, linlayer )\n",
    "            self.seq_model.add_module(\"relu_%d\" % i, nn.ReLU())\n",
    "            self.seq_model.add_module(\"batch_norm_%d\" % i, nn.BatchNorm1d(layer_sizes[i+1]))\n",
    "            self.seq_model.add_module(\"drop_out_%d\" % i, nn.Dropout(drop_pct))\n",
    "            \n",
    "        out_lin = nn.Linear(layer_sizes[-1], output_dim)\n",
    "        kaiming_normal(out_lin.weight.data)            \n",
    "        self.seq_model.add_module(\"output\", out_lin)\n",
    "        \n",
    "        # initialize the weights of linear layers\n",
    "        for ly in self.seq_model:\n",
    "            if type(ly) == nn.Linear:\n",
    "                kaiming_normal(ly.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # lookup embeddings\n",
    "        lkups = [emb(x[:,idx]) for idx, emb in enumerate(self.embs)]\n",
    "        glued = torch.cat(lkups,1)\n",
    "        x = self.emb_drop(glued)\n",
    "        x = self.seq_model(x)\n",
    "        if self.output_dim == 1:\n",
    "            x = F.sigmoid(x)\n",
    "        else:\n",
    "            x = F.log_softmax(x, output_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "complete ...\n",
      "formatting ...\n",
      "imputing missing values ...\n",
      "(18255, 1122) (18255,)\n",
      "converting to category ...\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "calculating cardinality\n",
      "remapping columns to int\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "X, y = load_xy_data()\n",
    "X_mapped, mappers, categorical_stats, emb_szs = get_mappers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.from_numpy(X_mapped.head(18200).as_matrix())\n",
    "y_tensor = torch.from_numpy(y[:18200]).view(-1,1)\n",
    "\n",
    "bz = 50\n",
    "train = data_utils.TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=bz, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 3 Layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of feats: 10\n",
      "total embedding parameters 182\n",
      "learning rate 0.010000\n",
      "[1/4] - 1200/18200 loss: 21.189893, acc: 0.810400\n",
      "[1/4] - 2450/18200 loss: 18.539863, acc: 0.828000\n",
      "[1/4] - 3700/18200 loss: 17.831759, acc: 0.850400\n",
      "[1/4] - 4950/18200 loss: 15.972036, acc: 0.874400\n",
      "[1/4] - 6200/18200 loss: 15.808735, acc: 0.867200\n",
      "[1/4] - 7450/18200 loss: 16.199249, acc: 0.856000\n",
      "[1/4] - 8700/18200 loss: 15.310520, acc: 0.864800\n",
      "[1/4] - 9950/18200 loss: 16.191557, acc: 0.855200\n",
      "[1/4] - 11200/18200 loss: 15.789908, acc: 0.865600\n",
      "[1/4] - 12450/18200 loss: 16.235570, acc: 0.864800\n",
      "[1/4] - 13700/18200 loss: 16.104269, acc: 0.867200\n",
      "[1/4] - 14950/18200 loss: 15.081647, acc: 0.872800\n",
      "[1/4] - 16200/18200 loss: 15.610961, acc: 0.880000\n",
      "[1/4] - 17450/18200 loss: 13.738026, acc: 0.890400\n",
      "[2/4] - 1200/18200 loss: 14.117018, acc: 0.885600\n",
      "[2/4] - 2450/18200 loss: 13.789540, acc: 0.880800\n",
      "[2/4] - 3700/18200 loss: 15.519359, acc: 0.870400\n",
      "[2/4] - 4950/18200 loss: 13.425933, acc: 0.893600\n",
      "[2/4] - 6200/18200 loss: 15.147702, acc: 0.880800\n",
      "[2/4] - 7450/18200 loss: 16.174259, acc: 0.865600\n",
      "[2/4] - 8700/18200 loss: 15.086689, acc: 0.874400\n",
      "[2/4] - 9950/18200 loss: 14.015874, acc: 0.890400\n",
      "[2/4] - 11200/18200 loss: 14.976972, acc: 0.866400\n",
      "[2/4] - 12450/18200 loss: 12.947307, acc: 0.892000\n",
      "[2/4] - 13700/18200 loss: 14.912366, acc: 0.873600\n",
      "[2/4] - 14950/18200 loss: 14.926454, acc: 0.879200\n",
      "[2/4] - 16200/18200 loss: 12.303884, acc: 0.904000\n",
      "[2/4] - 17450/18200 loss: 15.152421, acc: 0.876800\n",
      "[3/4] - 1200/18200 loss: 12.425295, acc: 0.888000\n",
      "[3/4] - 2450/18200 loss: 12.895779, acc: 0.892000\n",
      "[3/4] - 3700/18200 loss: 13.118687, acc: 0.889600\n",
      "[3/4] - 4950/18200 loss: 13.861534, acc: 0.876800\n",
      "[3/4] - 6200/18200 loss: 13.492969, acc: 0.883200\n",
      "[3/4] - 7450/18200 loss: 14.208922, acc: 0.872000\n",
      "[3/4] - 8700/18200 loss: 13.222299, acc: 0.895200\n",
      "[3/4] - 9950/18200 loss: 12.747508, acc: 0.894400\n",
      "[3/4] - 11200/18200 loss: 13.571382, acc: 0.902400\n",
      "[3/4] - 12450/18200 loss: 13.995162, acc: 0.882400\n",
      "[3/4] - 13700/18200 loss: 13.213915, acc: 0.890400\n",
      "[3/4] - 14950/18200 loss: 12.644785, acc: 0.896000\n",
      "[3/4] - 16200/18200 loss: 13.598156, acc: 0.888800\n",
      "[3/4] - 17450/18200 loss: 13.364379, acc: 0.892000\n",
      "[4/4] - 1200/18200 loss: 12.434920, acc: 0.892000\n",
      "[4/4] - 2450/18200 loss: 12.067058, acc: 0.896800\n",
      "[4/4] - 3700/18200 loss: 13.353987, acc: 0.884000\n",
      "[4/4] - 4950/18200 loss: 13.093304, acc: 0.894400\n",
      "[4/4] - 6200/18200 loss: 13.464104, acc: 0.880000\n",
      "[4/4] - 7450/18200 loss: 11.896166, acc: 0.900000\n",
      "[4/4] - 8700/18200 loss: 12.627190, acc: 0.889600\n",
      "[4/4] - 9950/18200 loss: 11.435853, acc: 0.900800\n",
      "[4/4] - 11200/18200 loss: 12.278518, acc: 0.902400\n",
      "[4/4] - 12450/18200 loss: 13.639473, acc: 0.888000\n",
      "[4/4] - 13700/18200 loss: 13.412979, acc: 0.890400\n",
      "[4/4] - 14950/18200 loss: 12.805745, acc: 0.897600\n",
      "[4/4] - 16200/18200 loss: 12.926276, acc: 0.886400\n",
      "[4/4] - 17450/18200 loss: 11.934048, acc: 0.900800\n",
      "learning rate 0.003000\n",
      "[1/4] - 1200/18200 loss: 10.121927, acc: 0.916000\n",
      "[1/4] - 2450/18200 loss: 11.430570, acc: 0.897600\n",
      "[1/4] - 3700/18200 loss: 10.886646, acc: 0.901600\n",
      "[1/4] - 4950/18200 loss: 11.968841, acc: 0.901600\n",
      "[1/4] - 6200/18200 loss: 10.347527, acc: 0.912800\n",
      "[1/4] - 7450/18200 loss: 9.808543, acc: 0.913600\n",
      "[1/4] - 8700/18200 loss: 10.361208, acc: 0.910400\n",
      "[1/4] - 9950/18200 loss: 11.219605, acc: 0.909600\n",
      "[1/4] - 11200/18200 loss: 10.847438, acc: 0.912000\n",
      "[1/4] - 12450/18200 loss: 10.061817, acc: 0.915200\n",
      "[1/4] - 13700/18200 loss: 10.383322, acc: 0.924000\n",
      "[1/4] - 14950/18200 loss: 10.148526, acc: 0.921600\n",
      "[1/4] - 16200/18200 loss: 10.605970, acc: 0.915200\n",
      "[1/4] - 17450/18200 loss: 12.170699, acc: 0.903200\n",
      "[2/4] - 1200/18200 loss: 10.501185, acc: 0.916800\n",
      "[2/4] - 2450/18200 loss: 9.457186, acc: 0.913600\n",
      "[2/4] - 3700/18200 loss: 9.331969, acc: 0.916000\n",
      "[2/4] - 4950/18200 loss: 10.697010, acc: 0.907200\n",
      "[2/4] - 6200/18200 loss: 10.507545, acc: 0.911200\n",
      "[2/4] - 7450/18200 loss: 9.897519, acc: 0.917600\n",
      "[2/4] - 8700/18200 loss: 10.775165, acc: 0.904800\n",
      "[2/4] - 9950/18200 loss: 10.584570, acc: 0.911200\n",
      "[2/4] - 11200/18200 loss: 9.183610, acc: 0.916000\n",
      "[2/4] - 12450/18200 loss: 9.511291, acc: 0.919200\n",
      "[2/4] - 13700/18200 loss: 10.175253, acc: 0.915200\n",
      "[2/4] - 14950/18200 loss: 9.523570, acc: 0.918400\n",
      "[2/4] - 16200/18200 loss: 10.687072, acc: 0.914400\n",
      "[2/4] - 17450/18200 loss: 9.045498, acc: 0.925600\n",
      "[3/4] - 1200/18200 loss: 9.944896, acc: 0.913600\n",
      "[3/4] - 2450/18200 loss: 10.705915, acc: 0.907200\n",
      "[3/4] - 3700/18200 loss: 8.755300, acc: 0.924800\n",
      "[3/4] - 4950/18200 loss: 9.150216, acc: 0.930400\n",
      "[3/4] - 6200/18200 loss: 9.633625, acc: 0.908000\n",
      "[3/4] - 7450/18200 loss: 12.122561, acc: 0.899200\n",
      "[3/4] - 8700/18200 loss: 9.318731, acc: 0.925600\n",
      "[3/4] - 9950/18200 loss: 9.462756, acc: 0.914400\n",
      "[3/4] - 11200/18200 loss: 10.825749, acc: 0.911200\n",
      "[3/4] - 12450/18200 loss: 10.194390, acc: 0.922400\n",
      "[3/4] - 13700/18200 loss: 9.365197, acc: 0.912800\n",
      "[3/4] - 14950/18200 loss: 10.713966, acc: 0.904000\n",
      "[3/4] - 16200/18200 loss: 10.219262, acc: 0.916800\n",
      "[3/4] - 17450/18200 loss: 9.597624, acc: 0.912000\n",
      "[4/4] - 1200/18200 loss: 8.851285, acc: 0.924800\n",
      "[4/4] - 2450/18200 loss: 8.438747, acc: 0.920000\n",
      "[4/4] - 3700/18200 loss: 9.682035, acc: 0.908800\n",
      "[4/4] - 4950/18200 loss: 8.504750, acc: 0.929600\n",
      "[4/4] - 6200/18200 loss: 8.998619, acc: 0.917600\n",
      "[4/4] - 7450/18200 loss: 9.368920, acc: 0.925600\n",
      "[4/4] - 8700/18200 loss: 9.310181, acc: 0.916800\n",
      "[4/4] - 9950/18200 loss: 10.734622, acc: 0.905600\n",
      "[4/4] - 11200/18200 loss: 8.810943, acc: 0.926400\n",
      "[4/4] - 12450/18200 loss: 9.872661, acc: 0.912800\n",
      "[4/4] - 13700/18200 loss: 10.440686, acc: 0.908000\n",
      "[4/4] - 14950/18200 loss: 10.966558, acc: 0.912000\n",
      "[4/4] - 16200/18200 loss: 9.600698, acc: 0.918400\n",
      "[4/4] - 17450/18200 loss: 10.810255, acc: 0.906400\n",
      "learning rate 0.001000\n",
      "[1/4] - 1200/18200 loss: 8.566035, acc: 0.931200\n",
      "[1/4] - 2450/18200 loss: 8.247342, acc: 0.932800\n",
      "[1/4] - 3700/18200 loss: 8.747769, acc: 0.924000\n",
      "[1/4] - 4950/18200 loss: 9.137819, acc: 0.916800\n",
      "[1/4] - 6200/18200 loss: 8.085491, acc: 0.928800\n",
      "[1/4] - 7450/18200 loss: 7.415747, acc: 0.936000\n",
      "[1/4] - 8700/18200 loss: 9.270598, acc: 0.922400\n",
      "[1/4] - 9950/18200 loss: 7.916839, acc: 0.924000\n",
      "[1/4] - 11200/18200 loss: 9.792103, acc: 0.917600\n",
      "[1/4] - 12450/18200 loss: 9.252981, acc: 0.919200\n",
      "[1/4] - 13700/18200 loss: 8.076907, acc: 0.932800\n",
      "[1/4] - 14950/18200 loss: 8.791800, acc: 0.928800\n",
      "[1/4] - 16200/18200 loss: 7.892736, acc: 0.934400\n",
      "[1/4] - 17450/18200 loss: 8.567094, acc: 0.924000\n",
      "[2/4] - 1200/18200 loss: 8.180200, acc: 0.934400\n",
      "[2/4] - 2450/18200 loss: 8.425356, acc: 0.931200\n",
      "[2/4] - 3700/18200 loss: 8.973769, acc: 0.921600\n",
      "[2/4] - 4950/18200 loss: 8.827493, acc: 0.927200\n",
      "[2/4] - 6200/18200 loss: 8.141050, acc: 0.928800\n",
      "[2/4] - 7450/18200 loss: 8.678970, acc: 0.927200\n",
      "[2/4] - 8700/18200 loss: 8.476797, acc: 0.924000\n",
      "[2/4] - 9950/18200 loss: 8.111726, acc: 0.930400\n",
      "[2/4] - 11200/18200 loss: 8.002997, acc: 0.930400\n",
      "[2/4] - 12450/18200 loss: 8.618608, acc: 0.923200\n",
      "[2/4] - 13700/18200 loss: 8.114571, acc: 0.932000\n",
      "[2/4] - 14950/18200 loss: 9.214447, acc: 0.920800\n",
      "[2/4] - 16200/18200 loss: 8.195224, acc: 0.927200\n",
      "[2/4] - 17450/18200 loss: 10.015217, acc: 0.912000\n",
      "[3/4] - 1200/18200 loss: 8.188868, acc: 0.924800\n",
      "[3/4] - 2450/18200 loss: 8.739179, acc: 0.924000\n",
      "[3/4] - 3700/18200 loss: 7.508770, acc: 0.942400\n",
      "[3/4] - 4950/18200 loss: 9.392480, acc: 0.914400\n",
      "[3/4] - 6200/18200 loss: 8.208228, acc: 0.925600\n",
      "[3/4] - 7450/18200 loss: 8.551387, acc: 0.924000\n",
      "[3/4] - 8700/18200 loss: 7.514390, acc: 0.934400\n",
      "[3/4] - 9950/18200 loss: 8.427053, acc: 0.932800\n",
      "[3/4] - 11200/18200 loss: 7.152497, acc: 0.929600\n",
      "[3/4] - 12450/18200 loss: 7.249347, acc: 0.936800\n",
      "[3/4] - 13700/18200 loss: 8.385856, acc: 0.924000\n",
      "[3/4] - 14950/18200 loss: 9.095757, acc: 0.917600\n",
      "[3/4] - 16200/18200 loss: 8.591910, acc: 0.926400\n",
      "[3/4] - 17450/18200 loss: 9.264852, acc: 0.915200\n",
      "[4/4] - 1200/18200 loss: 8.868961, acc: 0.920800\n",
      "[4/4] - 2450/18200 loss: 7.582642, acc: 0.929600\n",
      "[4/4] - 3700/18200 loss: 8.130842, acc: 0.929600\n",
      "[4/4] - 4950/18200 loss: 7.931274, acc: 0.929600\n",
      "[4/4] - 6200/18200 loss: 9.766843, acc: 0.914400\n",
      "[4/4] - 7450/18200 loss: 7.469522, acc: 0.931200\n",
      "[4/4] - 8700/18200 loss: 7.258407, acc: 0.932800\n",
      "[4/4] - 9950/18200 loss: 8.024816, acc: 0.929600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/4] - 11200/18200 loss: 8.900819, acc: 0.929600\n",
      "[4/4] - 12450/18200 loss: 9.102890, acc: 0.926400\n",
      "[4/4] - 13700/18200 loss: 8.080157, acc: 0.929600\n",
      "[4/4] - 14950/18200 loss: 9.017907, acc: 0.928800\n",
      "[4/4] - 16200/18200 loss: 7.161745, acc: 0.932000\n",
      "[4/4] - 17450/18200 loss: 7.846181, acc: 0.932000\n",
      "learning rate 0.000300\n",
      "[1/4] - 1200/18200 loss: 8.839418, acc: 0.922400\n",
      "[1/4] - 2450/18200 loss: 8.610362, acc: 0.929600\n",
      "[1/4] - 3700/18200 loss: 9.829206, acc: 0.911200\n",
      "[1/4] - 4950/18200 loss: 6.982679, acc: 0.948000\n",
      "[1/4] - 6200/18200 loss: 8.290683, acc: 0.922400\n",
      "[1/4] - 7450/18200 loss: 6.870121, acc: 0.945600\n",
      "[1/4] - 8700/18200 loss: 7.850580, acc: 0.936000\n",
      "[1/4] - 9950/18200 loss: 7.348506, acc: 0.936800\n",
      "[1/4] - 11200/18200 loss: 8.296039, acc: 0.932800\n",
      "[1/4] - 12450/18200 loss: 8.304887, acc: 0.923200\n",
      "[1/4] - 13700/18200 loss: 7.403085, acc: 0.931200\n",
      "[1/4] - 14950/18200 loss: 7.469584, acc: 0.935200\n",
      "[1/4] - 16200/18200 loss: 7.557291, acc: 0.940000\n",
      "[1/4] - 17450/18200 loss: 6.575661, acc: 0.939200\n",
      "[2/4] - 1200/18200 loss: 7.840777, acc: 0.936000\n",
      "[2/4] - 2450/18200 loss: 7.148370, acc: 0.936000\n",
      "[2/4] - 3700/18200 loss: 7.742261, acc: 0.926400\n",
      "[2/4] - 4950/18200 loss: 7.329272, acc: 0.933600\n",
      "[2/4] - 6200/18200 loss: 7.278050, acc: 0.935200\n",
      "[2/4] - 7450/18200 loss: 8.367603, acc: 0.934400\n",
      "[2/4] - 8700/18200 loss: 8.530107, acc: 0.926400\n",
      "[2/4] - 9950/18200 loss: 7.970771, acc: 0.927200\n",
      "[2/4] - 11200/18200 loss: 9.157643, acc: 0.916000\n",
      "[2/4] - 12450/18200 loss: 7.501984, acc: 0.930400\n",
      "[2/4] - 13700/18200 loss: 7.948047, acc: 0.932800\n",
      "[2/4] - 14950/18200 loss: 8.190667, acc: 0.927200\n",
      "[2/4] - 16200/18200 loss: 7.559296, acc: 0.928800\n",
      "[2/4] - 17450/18200 loss: 7.644849, acc: 0.931200\n",
      "[3/4] - 1200/18200 loss: 7.578134, acc: 0.933600\n",
      "[3/4] - 2450/18200 loss: 6.960681, acc: 0.932000\n",
      "[3/4] - 3700/18200 loss: 7.420527, acc: 0.935200\n",
      "[3/4] - 4950/18200 loss: 8.193106, acc: 0.931200\n",
      "[3/4] - 6200/18200 loss: 7.257852, acc: 0.937600\n",
      "[3/4] - 7450/18200 loss: 7.194648, acc: 0.949600\n",
      "[3/4] - 8700/18200 loss: 8.083601, acc: 0.931200\n",
      "[3/4] - 9950/18200 loss: 7.279376, acc: 0.936000\n",
      "[3/4] - 11200/18200 loss: 6.578562, acc: 0.948000\n",
      "[3/4] - 12450/18200 loss: 7.352011, acc: 0.934400\n",
      "[3/4] - 13700/18200 loss: 7.693115, acc: 0.935200\n",
      "[3/4] - 14950/18200 loss: 8.403745, acc: 0.932800\n",
      "[3/4] - 16200/18200 loss: 7.872757, acc: 0.929600\n",
      "[3/4] - 17450/18200 loss: 8.963514, acc: 0.924000\n",
      "[4/4] - 1200/18200 loss: 6.693422, acc: 0.939200\n",
      "[4/4] - 2450/18200 loss: 8.501127, acc: 0.927200\n",
      "[4/4] - 3700/18200 loss: 8.086563, acc: 0.932800\n",
      "[4/4] - 4950/18200 loss: 7.412441, acc: 0.936000\n",
      "[4/4] - 6200/18200 loss: 6.928933, acc: 0.943200\n",
      "[4/4] - 7450/18200 loss: 7.822164, acc: 0.930400\n",
      "[4/4] - 8700/18200 loss: 6.672812, acc: 0.944800\n",
      "[4/4] - 9950/18200 loss: 7.554911, acc: 0.930400\n",
      "[4/4] - 11200/18200 loss: 8.018332, acc: 0.930400\n",
      "[4/4] - 12450/18200 loss: 6.540867, acc: 0.941600\n",
      "[4/4] - 13700/18200 loss: 7.357249, acc: 0.929600\n",
      "[4/4] - 14950/18200 loss: 7.496956, acc: 0.932000\n",
      "[4/4] - 16200/18200 loss: 8.272452, acc: 0.931200\n",
      "[4/4] - 17450/18200 loss: 8.348027, acc: 0.924000\n",
      "learning rate 0.000100\n",
      "[1/4] - 1200/18200 loss: 8.678076, acc: 0.920800\n",
      "[1/4] - 2450/18200 loss: 7.175141, acc: 0.933600\n",
      "[1/4] - 3700/18200 loss: 7.687910, acc: 0.930400\n",
      "[1/4] - 4950/18200 loss: 6.764659, acc: 0.940800\n",
      "[1/4] - 6200/18200 loss: 6.941247, acc: 0.946400\n",
      "[1/4] - 7450/18200 loss: 8.640083, acc: 0.928000\n",
      "[1/4] - 8700/18200 loss: 7.302882, acc: 0.936800\n",
      "[1/4] - 9950/18200 loss: 7.480426, acc: 0.936800\n",
      "[1/4] - 11200/18200 loss: 6.625398, acc: 0.947200\n",
      "[1/4] - 12450/18200 loss: 8.249818, acc: 0.925600\n",
      "[1/4] - 13700/18200 loss: 7.462494, acc: 0.940000\n",
      "[1/4] - 14950/18200 loss: 7.052705, acc: 0.941600\n",
      "[1/4] - 16200/18200 loss: 7.179095, acc: 0.938400\n",
      "[1/4] - 17450/18200 loss: 7.643939, acc: 0.929600\n",
      "[2/4] - 1200/18200 loss: 7.491759, acc: 0.936000\n",
      "[2/4] - 2450/18200 loss: 8.426070, acc: 0.923200\n",
      "[2/4] - 3700/18200 loss: 6.700736, acc: 0.943200\n",
      "[2/4] - 4950/18200 loss: 7.855991, acc: 0.929600\n",
      "[2/4] - 6200/18200 loss: 8.048428, acc: 0.925600\n",
      "[2/4] - 7450/18200 loss: 6.934634, acc: 0.940800\n",
      "[2/4] - 8700/18200 loss: 7.259000, acc: 0.939200\n",
      "[2/4] - 9950/18200 loss: 7.830397, acc: 0.926400\n",
      "[2/4] - 11200/18200 loss: 7.415766, acc: 0.942400\n",
      "[2/4] - 12450/18200 loss: 7.915833, acc: 0.936000\n",
      "[2/4] - 13700/18200 loss: 8.516354, acc: 0.925600\n",
      "[2/4] - 14950/18200 loss: 7.062915, acc: 0.941600\n",
      "[2/4] - 16200/18200 loss: 7.308439, acc: 0.933600\n",
      "[2/4] - 17450/18200 loss: 7.685583, acc: 0.932000\n",
      "[3/4] - 1200/18200 loss: 8.149900, acc: 0.931200\n",
      "[3/4] - 2450/18200 loss: 8.056262, acc: 0.936800\n",
      "[3/4] - 3700/18200 loss: 8.532607, acc: 0.925600\n",
      "[3/4] - 4950/18200 loss: 8.059982, acc: 0.926400\n",
      "[3/4] - 6200/18200 loss: 6.754688, acc: 0.939200\n",
      "[3/4] - 7450/18200 loss: 7.411583, acc: 0.936800\n",
      "[3/4] - 8700/18200 loss: 7.644583, acc: 0.929600\n",
      "[3/4] - 9950/18200 loss: 6.633217, acc: 0.941600\n",
      "[3/4] - 11200/18200 loss: 6.440269, acc: 0.950400\n",
      "[3/4] - 12450/18200 loss: 7.758018, acc: 0.932800\n",
      "[3/4] - 13700/18200 loss: 7.322760, acc: 0.933600\n",
      "[3/4] - 14950/18200 loss: 7.071898, acc: 0.937600\n",
      "[3/4] - 16200/18200 loss: 7.980680, acc: 0.935200\n",
      "[3/4] - 17450/18200 loss: 8.634813, acc: 0.931200\n",
      "[4/4] - 1200/18200 loss: 6.805946, acc: 0.936000\n",
      "[4/4] - 2450/18200 loss: 6.594249, acc: 0.941600\n",
      "[4/4] - 3700/18200 loss: 6.855515, acc: 0.930400\n",
      "[4/4] - 4950/18200 loss: 7.488926, acc: 0.935200\n",
      "[4/4] - 6200/18200 loss: 7.726237, acc: 0.925600\n",
      "[4/4] - 7450/18200 loss: 9.028151, acc: 0.924800\n",
      "[4/4] - 8700/18200 loss: 7.939224, acc: 0.927200\n",
      "[4/4] - 9950/18200 loss: 6.808257, acc: 0.933600\n",
      "[4/4] - 11200/18200 loss: 7.388053, acc: 0.940800\n",
      "[4/4] - 12450/18200 loss: 7.223029, acc: 0.931200\n",
      "[4/4] - 13700/18200 loss: 8.674807, acc: 0.932800\n",
      "[4/4] - 14950/18200 loss: 7.332355, acc: 0.936000\n",
      "[4/4] - 16200/18200 loss: 6.945980, acc: 0.942400\n",
      "[4/4] - 17450/18200 loss: 7.264817, acc: 0.940000\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [1000, 300, 100]\n",
    "\n",
    "# initialize \n",
    "model = EmbeddingModel(emb_szs=emb_szs, layer_sizes=layer_sizes, output_dim=1, drop_pct=0.3, emb_drop_pct=0.3)\n",
    "model.seq_model\n",
    "\n",
    "n_epochs = 4\n",
    "wd=1e-5\n",
    "avg_loss = []\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=wd)\n",
    "loss_fn = torch.nn.BCELoss(size_average=False)\n",
    "\n",
    "\n",
    "for learning_rate in [0.01, 0.003, 0.001, 0.0003, 0.0001]:\n",
    "    print('learning rate %f' % learning_rate)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = learning_rate\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        \n",
    "        train_dl = iter(train_loader)\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(train_dl):\n",
    "            data, labels = batch\n",
    "            bz = data.size()[0]\n",
    "\n",
    "            data_var = Variable(data)\n",
    "            label_var = Variable(labels.float(), requires_grad=False)\n",
    "\n",
    "            y_pred = model(data_var)\n",
    "            y_pred_hard = y_pred > 0.5\n",
    "            correct = (label_var.view(-1,1).eq(y_pred_hard.float())).sum()\n",
    "            running_correct += correct.float().data    \n",
    "\n",
    "            loss = loss_fn(y_pred, label_var)\n",
    "            running_loss += loss.data[0]\n",
    "\n",
    "            if i % 25 == 24:\n",
    "                avg_loss.append(running_loss/25)\n",
    "                acc = running_correct/50/25.\n",
    "                print('[%d/%d] - %d/%d loss: %f, acc: %f' %(epoch+1, n_epochs, i*bz, 18200, running_loss/25, acc))\n",
    "                \n",
    "                running_loss = 0\n",
    "                running_correct = 0\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a0f8b89b0>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a100ca630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for emb in model.embs:\n",
    "#     print(emb.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Change X into Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = mappers.keys()\n",
    "emb_mtx = {}\n",
    "for field, emb in zip(keys, model.embs):\n",
    "    emb_mtx[field] = emb.weight.data.numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "complete ...\n",
      "formatting ...\n",
      "imputing missing values ...\n",
      "complete\n",
      "applying embeddings\n",
      "combining dfs\n",
      "applying embeddings\n",
      "combining dfs\n"
     ]
    }
   ],
   "source": [
    "def get_train():\n",
    "    print('loading data ...')\n",
    "    df = pd.read_csv('./train.csv', low_memory=False)\n",
    "    test = pd.read_csv('./test.csv', low_memory=False)\n",
    "    print('complete ...')\n",
    "\n",
    "    print('formatting ...')\n",
    "    drop_cols = [col for col in df.columns if 'OTHERS' in col] + [col for col in df.columns if 'REC' in col] + ['LN2_RIndLngBEOth', 'LN2_WIndLngBEOth', 'train_id']\n",
    "    drop_cols_test = [col for col in df.columns if 'OTHERS' in col] + [col for col in df.columns if 'REC' in col] + ['LN2_RIndLngBEOth', 'LN2_WIndLngBEOth', 'test_id']\n",
    "    \n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "    test.drop(columns=drop_cols_test, inplace=True)\n",
    "    \n",
    "    columns = [col for col in df.columns if col != 'is_female']\n",
    "    y = df['is_female'].values\n",
    "    X = df[columns].copy()\n",
    "    X_test = test[columns].copy()\n",
    "\n",
    "    \n",
    "    print('imputing missing values ...')\n",
    "    X.fillna(-1, inplace=True)\n",
    "    X_test.fillna(-1, inplace=True)\n",
    "    \n",
    "    mappers = {}\n",
    "    print('complete')\n",
    "    return X, y, X_test\n",
    "\n",
    "\n",
    "def get_emb_df(X, emb_mtx):\n",
    "    mini_dfs = []\n",
    "    \n",
    "    print('applying embeddings')\n",
    "    for col in X.columns.values:\n",
    "        idxs = X[col].map(mappers[col])\n",
    "        \n",
    "        # fill nones with global mean\n",
    "        idxs[idxs.isna()] = max(idxs)+1\n",
    "        idxs = np.array(idxs, dtype = int)\n",
    "        \n",
    "        # get embedding matrix\n",
    "        mtx = emb_mtx[col]\n",
    "        \n",
    "        # calculate global mean for missing values\n",
    "        glb_mean = np.mean(mtx,axis=0)        \n",
    "        \n",
    "        # add global mean to bottom of matrix\n",
    "        mtx = np.concatenate([mtx, glb_mean.reshape(1,-1)], axis=0)\n",
    "        \n",
    "        # create dataframe\n",
    "        jf = pd.DataFrame(mtx[idxs, :])\n",
    "        jf.columns = [col+'_%d' %i for i in jf.columns]\n",
    "        \n",
    "        # append\n",
    "        mini_dfs.append(jf)\n",
    "    print('combining dfs')\n",
    "    out_df = pd.concat(mini_dfs, axis=1)\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "X, y, X_test = get_train()\n",
    "X_emb = get_emb_df(X[top_f], emb_mtx)\n",
    "X_emb_test = get_emb_df(X_test[top_f], emb_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.962216\tvalidation_1-auc:0.956286\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 50 rounds.\n",
      "[1]\tvalidation_0-auc:0.96792\tvalidation_1-auc:0.962703\n",
      "[2]\tvalidation_0-auc:0.971664\tvalidation_1-auc:0.967035\n",
      "[3]\tvalidation_0-auc:0.973383\tvalidation_1-auc:0.968572\n",
      "[4]\tvalidation_0-auc:0.974232\tvalidation_1-auc:0.969471\n",
      "[5]\tvalidation_0-auc:0.975184\tvalidation_1-auc:0.969679\n",
      "[6]\tvalidation_0-auc:0.976124\tvalidation_1-auc:0.970757\n",
      "[7]\tvalidation_0-auc:0.97666\tvalidation_1-auc:0.971304\n",
      "[8]\tvalidation_0-auc:0.9778\tvalidation_1-auc:0.972294\n",
      "[9]\tvalidation_0-auc:0.978755\tvalidation_1-auc:0.972827\n",
      "[10]\tvalidation_0-auc:0.979448\tvalidation_1-auc:0.973299\n",
      "[11]\tvalidation_0-auc:0.980177\tvalidation_1-auc:0.973561\n",
      "[12]\tvalidation_0-auc:0.980733\tvalidation_1-auc:0.973708\n",
      "[13]\tvalidation_0-auc:0.981321\tvalidation_1-auc:0.973646\n",
      "[14]\tvalidation_0-auc:0.981983\tvalidation_1-auc:0.973894\n",
      "[15]\tvalidation_0-auc:0.982322\tvalidation_1-auc:0.974066\n",
      "[16]\tvalidation_0-auc:0.982831\tvalidation_1-auc:0.974264\n",
      "[17]\tvalidation_0-auc:0.983387\tvalidation_1-auc:0.974421\n",
      "[18]\tvalidation_0-auc:0.983883\tvalidation_1-auc:0.974705\n",
      "[19]\tvalidation_0-auc:0.984314\tvalidation_1-auc:0.974887\n",
      "[20]\tvalidation_0-auc:0.984821\tvalidation_1-auc:0.975103\n",
      "[21]\tvalidation_0-auc:0.98537\tvalidation_1-auc:0.975263\n",
      "[22]\tvalidation_0-auc:0.985775\tvalidation_1-auc:0.975476\n",
      "[23]\tvalidation_0-auc:0.986067\tvalidation_1-auc:0.975594\n",
      "[24]\tvalidation_0-auc:0.986614\tvalidation_1-auc:0.975842\n",
      "[25]\tvalidation_0-auc:0.986887\tvalidation_1-auc:0.975926\n",
      "[26]\tvalidation_0-auc:0.987326\tvalidation_1-auc:0.976153\n",
      "[27]\tvalidation_0-auc:0.987659\tvalidation_1-auc:0.976265\n",
      "[28]\tvalidation_0-auc:0.988034\tvalidation_1-auc:0.976379\n",
      "[29]\tvalidation_0-auc:0.988429\tvalidation_1-auc:0.976335\n",
      "[30]\tvalidation_0-auc:0.988792\tvalidation_1-auc:0.976422\n",
      "[31]\tvalidation_0-auc:0.989146\tvalidation_1-auc:0.976545\n",
      "[32]\tvalidation_0-auc:0.989517\tvalidation_1-auc:0.976596\n",
      "[33]\tvalidation_0-auc:0.989814\tvalidation_1-auc:0.976704\n",
      "[34]\tvalidation_0-auc:0.990057\tvalidation_1-auc:0.976774\n",
      "[35]\tvalidation_0-auc:0.990404\tvalidation_1-auc:0.976892\n",
      "[36]\tvalidation_0-auc:0.990772\tvalidation_1-auc:0.976951\n",
      "[37]\tvalidation_0-auc:0.991031\tvalidation_1-auc:0.977139\n",
      "[38]\tvalidation_0-auc:0.991284\tvalidation_1-auc:0.977284\n",
      "[39]\tvalidation_0-auc:0.991587\tvalidation_1-auc:0.977307\n",
      "[40]\tvalidation_0-auc:0.991819\tvalidation_1-auc:0.977426\n",
      "[41]\tvalidation_0-auc:0.992075\tvalidation_1-auc:0.977502\n",
      "[42]\tvalidation_0-auc:0.992303\tvalidation_1-auc:0.97758\n",
      "[43]\tvalidation_0-auc:0.99249\tvalidation_1-auc:0.977656\n",
      "[44]\tvalidation_0-auc:0.992841\tvalidation_1-auc:0.977805\n",
      "[45]\tvalidation_0-auc:0.993016\tvalidation_1-auc:0.977776\n",
      "[46]\tvalidation_0-auc:0.993257\tvalidation_1-auc:0.977818\n",
      "[47]\tvalidation_0-auc:0.993398\tvalidation_1-auc:0.977847\n",
      "[48]\tvalidation_0-auc:0.993523\tvalidation_1-auc:0.977901\n",
      "[49]\tvalidation_0-auc:0.993757\tvalidation_1-auc:0.977985\n",
      "[50]\tvalidation_0-auc:0.994016\tvalidation_1-auc:0.978032\n",
      "[51]\tvalidation_0-auc:0.994141\tvalidation_1-auc:0.978029\n",
      "[52]\tvalidation_0-auc:0.994358\tvalidation_1-auc:0.978143\n",
      "[53]\tvalidation_0-auc:0.994602\tvalidation_1-auc:0.978192\n",
      "[54]\tvalidation_0-auc:0.994794\tvalidation_1-auc:0.978288\n",
      "[55]\tvalidation_0-auc:0.995022\tvalidation_1-auc:0.978219\n",
      "[56]\tvalidation_0-auc:0.995103\tvalidation_1-auc:0.978217\n",
      "[57]\tvalidation_0-auc:0.9952\tvalidation_1-auc:0.97823\n",
      "[58]\tvalidation_0-auc:0.995421\tvalidation_1-auc:0.978373\n",
      "[59]\tvalidation_0-auc:0.995535\tvalidation_1-auc:0.978384\n",
      "[60]\tvalidation_0-auc:0.995799\tvalidation_1-auc:0.978445\n",
      "[61]\tvalidation_0-auc:0.995993\tvalidation_1-auc:0.97857\n",
      "[62]\tvalidation_0-auc:0.99614\tvalidation_1-auc:0.978627\n",
      "[63]\tvalidation_0-auc:0.996216\tvalidation_1-auc:0.978672\n",
      "[64]\tvalidation_0-auc:0.9963\tvalidation_1-auc:0.978687\n",
      "[65]\tvalidation_0-auc:0.996346\tvalidation_1-auc:0.978766\n",
      "[66]\tvalidation_0-auc:0.996412\tvalidation_1-auc:0.978819\n",
      "[67]\tvalidation_0-auc:0.996573\tvalidation_1-auc:0.978859\n",
      "[68]\tvalidation_0-auc:0.996656\tvalidation_1-auc:0.978868\n",
      "[69]\tvalidation_0-auc:0.996723\tvalidation_1-auc:0.978853\n",
      "[70]\tvalidation_0-auc:0.996825\tvalidation_1-auc:0.978885\n",
      "[71]\tvalidation_0-auc:0.996982\tvalidation_1-auc:0.978915\n",
      "[72]\tvalidation_0-auc:0.9971\tvalidation_1-auc:0.978872\n",
      "[73]\tvalidation_0-auc:0.997206\tvalidation_1-auc:0.978898\n",
      "[74]\tvalidation_0-auc:0.997244\tvalidation_1-auc:0.97893\n",
      "[75]\tvalidation_0-auc:0.99729\tvalidation_1-auc:0.978975\n",
      "[76]\tvalidation_0-auc:0.997411\tvalidation_1-auc:0.978957\n",
      "[77]\tvalidation_0-auc:0.997536\tvalidation_1-auc:0.978958\n",
      "[78]\tvalidation_0-auc:0.997601\tvalidation_1-auc:0.97903\n",
      "[79]\tvalidation_0-auc:0.997651\tvalidation_1-auc:0.979024\n",
      "[80]\tvalidation_0-auc:0.997679\tvalidation_1-auc:0.979013\n",
      "[81]\tvalidation_0-auc:0.997741\tvalidation_1-auc:0.978963\n",
      "[82]\tvalidation_0-auc:0.99778\tvalidation_1-auc:0.978931\n",
      "[83]\tvalidation_0-auc:0.99782\tvalidation_1-auc:0.978956\n",
      "[84]\tvalidation_0-auc:0.997923\tvalidation_1-auc:0.978914\n",
      "[85]\tvalidation_0-auc:0.99798\tvalidation_1-auc:0.978944\n",
      "[86]\tvalidation_0-auc:0.998066\tvalidation_1-auc:0.978926\n",
      "[87]\tvalidation_0-auc:0.998089\tvalidation_1-auc:0.978967\n",
      "[88]\tvalidation_0-auc:0.998118\tvalidation_1-auc:0.978965\n",
      "[89]\tvalidation_0-auc:0.998182\tvalidation_1-auc:0.978966\n",
      "[90]\tvalidation_0-auc:0.998214\tvalidation_1-auc:0.978937\n",
      "[91]\tvalidation_0-auc:0.998255\tvalidation_1-auc:0.978955\n",
      "[92]\tvalidation_0-auc:0.998318\tvalidation_1-auc:0.978984\n",
      "[93]\tvalidation_0-auc:0.99834\tvalidation_1-auc:0.978995\n",
      "[94]\tvalidation_0-auc:0.998447\tvalidation_1-auc:0.978981\n",
      "[95]\tvalidation_0-auc:0.998456\tvalidation_1-auc:0.978965\n",
      "[96]\tvalidation_0-auc:0.998511\tvalidation_1-auc:0.978982\n",
      "[97]\tvalidation_0-auc:0.998576\tvalidation_1-auc:0.978918\n",
      "[98]\tvalidation_0-auc:0.998607\tvalidation_1-auc:0.978892\n",
      "[99]\tvalidation_0-auc:0.998691\tvalidation_1-auc:0.978894\n",
      "[100]\tvalidation_0-auc:0.998703\tvalidation_1-auc:0.978904\n",
      "[101]\tvalidation_0-auc:0.998758\tvalidation_1-auc:0.97894\n",
      "[102]\tvalidation_0-auc:0.998773\tvalidation_1-auc:0.978964\n",
      "[103]\tvalidation_0-auc:0.998796\tvalidation_1-auc:0.9789\n",
      "[104]\tvalidation_0-auc:0.998843\tvalidation_1-auc:0.978904\n",
      "[105]\tvalidation_0-auc:0.998897\tvalidation_1-auc:0.978928\n",
      "[106]\tvalidation_0-auc:0.998958\tvalidation_1-auc:0.978967\n",
      "[107]\tvalidation_0-auc:0.998985\tvalidation_1-auc:0.979011\n",
      "[108]\tvalidation_0-auc:0.998997\tvalidation_1-auc:0.978982\n",
      "[109]\tvalidation_0-auc:0.999041\tvalidation_1-auc:0.978935\n",
      "[110]\tvalidation_0-auc:0.999068\tvalidation_1-auc:0.978911\n",
      "[111]\tvalidation_0-auc:0.999123\tvalidation_1-auc:0.978968\n",
      "[112]\tvalidation_0-auc:0.999148\tvalidation_1-auc:0.978935\n",
      "[113]\tvalidation_0-auc:0.999162\tvalidation_1-auc:0.978926\n",
      "[114]\tvalidation_0-auc:0.999208\tvalidation_1-auc:0.978951\n",
      "[115]\tvalidation_0-auc:0.999225\tvalidation_1-auc:0.978957\n",
      "[116]\tvalidation_0-auc:0.999245\tvalidation_1-auc:0.978979\n",
      "[117]\tvalidation_0-auc:0.999291\tvalidation_1-auc:0.978955\n",
      "[118]\tvalidation_0-auc:0.999327\tvalidation_1-auc:0.978935\n",
      "[119]\tvalidation_0-auc:0.999371\tvalidation_1-auc:0.97892\n",
      "[120]\tvalidation_0-auc:0.999395\tvalidation_1-auc:0.978876\n",
      "[121]\tvalidation_0-auc:0.999412\tvalidation_1-auc:0.978884\n",
      "[122]\tvalidation_0-auc:0.999452\tvalidation_1-auc:0.97889\n",
      "[123]\tvalidation_0-auc:0.999461\tvalidation_1-auc:0.978878\n",
      "[124]\tvalidation_0-auc:0.999478\tvalidation_1-auc:0.978874\n",
      "[125]\tvalidation_0-auc:0.999493\tvalidation_1-auc:0.978882\n",
      "[126]\tvalidation_0-auc:0.999504\tvalidation_1-auc:0.978887\n",
      "[127]\tvalidation_0-auc:0.999534\tvalidation_1-auc:0.978905\n",
      "[128]\tvalidation_0-auc:0.999556\tvalidation_1-auc:0.97891\n",
      "Stopping. Best iteration:\n",
      "[78]\tvalidation_0-auc:0.997601\tvalidation_1-auc:0.97903\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n",
       "       colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=6, min_child_weight=1, missing=None, n_estimators=500,\n",
       "       n_jobs=-1, nthread=None, objective='binary:logistic',\n",
       "       random_state=0, reg_alpha=0.1, reg_lambda=0.01, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_emb, y, test_size=0.3)\n",
    "m = xgb.XGBClassifier(max_depth = 6, \n",
    "                      learning_rate=0.1,\n",
    "                      reg_alpha=0.1, \n",
    "                      colsample_bytree = 0.5,\n",
    "                      colsample_bylevel = 0.5,\n",
    "                      reg_lambda=0.01, \n",
    "                      n_estimators=500,\n",
    "                      n_jobs = -1\n",
    "                     )\n",
    "\n",
    "m.fit(X_train, \n",
    "      y_train, \n",
    "      eval_set=([X_train,y_train],[X_val, y_val]),\n",
    "      eval_metric='auc',\n",
    "      early_stopping_rounds=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_hard = m.predict(X_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96713229252259658"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.equal(y, pred_hard))/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = m.predict_proba(X_emb_test)\n",
    "pred_df = pd.DataFrame(pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv('./sample_submission.csv')\n",
    "df_sub['is_female'] = pred_df.iloc[:,1]\n",
    "df_sub.to_csv('nn_embedding_9.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>is_female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.999973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.577027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.999694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.960567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.999954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.003924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.999921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.002181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.054982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.042659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.041054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.203959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.990427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.998099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.808709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.416846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.700501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.005385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.995882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.999890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.948432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.999927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.302825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.118968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.471753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.674230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.000570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.998126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27255</th>\n",
       "      <td>27255</td>\n",
       "      <td>0.003791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27256</th>\n",
       "      <td>27256</td>\n",
       "      <td>0.999945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27257</th>\n",
       "      <td>27257</td>\n",
       "      <td>0.008669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27258</th>\n",
       "      <td>27258</td>\n",
       "      <td>0.000435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27259</th>\n",
       "      <td>27259</td>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27260</th>\n",
       "      <td>27260</td>\n",
       "      <td>0.998066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27261</th>\n",
       "      <td>27261</td>\n",
       "      <td>0.842945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27262</th>\n",
       "      <td>27262</td>\n",
       "      <td>0.000996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27263</th>\n",
       "      <td>27263</td>\n",
       "      <td>0.547001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27264</th>\n",
       "      <td>27264</td>\n",
       "      <td>0.999064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27265</th>\n",
       "      <td>27265</td>\n",
       "      <td>0.999967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27266</th>\n",
       "      <td>27266</td>\n",
       "      <td>0.000292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27267</th>\n",
       "      <td>27267</td>\n",
       "      <td>0.982321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27268</th>\n",
       "      <td>27268</td>\n",
       "      <td>0.900201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27269</th>\n",
       "      <td>27269</td>\n",
       "      <td>0.999961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27270</th>\n",
       "      <td>27270</td>\n",
       "      <td>0.999955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27271</th>\n",
       "      <td>27271</td>\n",
       "      <td>0.002840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27272</th>\n",
       "      <td>27272</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27273</th>\n",
       "      <td>27273</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27274</th>\n",
       "      <td>27274</td>\n",
       "      <td>0.999841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27275</th>\n",
       "      <td>27275</td>\n",
       "      <td>0.960524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27276</th>\n",
       "      <td>27276</td>\n",
       "      <td>0.000783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27277</th>\n",
       "      <td>27277</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27278</th>\n",
       "      <td>27278</td>\n",
       "      <td>0.008043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27279</th>\n",
       "      <td>27279</td>\n",
       "      <td>0.035068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27280</th>\n",
       "      <td>27280</td>\n",
       "      <td>0.963585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27281</th>\n",
       "      <td>27281</td>\n",
       "      <td>0.997674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27282</th>\n",
       "      <td>27282</td>\n",
       "      <td>0.000172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27283</th>\n",
       "      <td>27283</td>\n",
       "      <td>0.031714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27284</th>\n",
       "      <td>27284</td>\n",
       "      <td>0.975369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27285 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_id  is_female\n",
       "0            0   0.999973\n",
       "1            1   0.001265\n",
       "2            2   0.577027\n",
       "3            3   0.999694\n",
       "4            4   0.960567\n",
       "5            5   0.999954\n",
       "6            6   0.003924\n",
       "7            7   0.999921\n",
       "8            8   0.002181\n",
       "9            9   0.054982\n",
       "10          10   0.042659\n",
       "11          11   0.041054\n",
       "12          12   0.203959\n",
       "13          13   0.990427\n",
       "14          14   0.998099\n",
       "15          15   0.808709\n",
       "16          16   0.416846\n",
       "17          17   0.700501\n",
       "18          18   0.005385\n",
       "19          19   0.995882\n",
       "20          20   0.000134\n",
       "21          21   0.999890\n",
       "22          22   0.948432\n",
       "23          23   0.999927\n",
       "24          24   0.302825\n",
       "25          25   0.118968\n",
       "26          26   0.471753\n",
       "27          27   0.674230\n",
       "28          28   0.000570\n",
       "29          29   0.998126\n",
       "...        ...        ...\n",
       "27255    27255   0.003791\n",
       "27256    27256   0.999945\n",
       "27257    27257   0.008669\n",
       "27258    27258   0.000435\n",
       "27259    27259   0.000182\n",
       "27260    27260   0.998066\n",
       "27261    27261   0.842945\n",
       "27262    27262   0.000996\n",
       "27263    27263   0.547001\n",
       "27264    27264   0.999064\n",
       "27265    27265   0.999967\n",
       "27266    27266   0.000292\n",
       "27267    27267   0.982321\n",
       "27268    27268   0.900201\n",
       "27269    27269   0.999961\n",
       "27270    27270   0.999955\n",
       "27271    27271   0.002840\n",
       "27272    27272   0.999997\n",
       "27273    27273   0.999995\n",
       "27274    27274   0.999841\n",
       "27275    27275   0.960524\n",
       "27276    27276   0.000783\n",
       "27277    27277   0.999982\n",
       "27278    27278   0.008043\n",
       "27279    27279   0.035068\n",
       "27280    27280   0.963585\n",
       "27281    27281   0.997674\n",
       "27282    27282   0.000172\n",
       "27283    27283   0.031714\n",
       "27284    27284   0.975369\n",
       "\n",
       "[27285 rows x 2 columns]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
